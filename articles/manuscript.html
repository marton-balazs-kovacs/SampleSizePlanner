<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>SampleSizePlanner: A Tool to Estimate and Justify Sample Size for Two-Group Studies • SampleSizePlanner</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="SampleSizePlanner: A Tool to Estimate and Justify Sample Size for Two-Group Studies">
<meta property="og:description" content="SampleSizePlanner">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">SampleSizePlanner</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/manuscript.html">SampleSizePlanner: A Tool to Estimate and Justify Sample Size for Two-Group Studies</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="manuscript_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>SampleSizePlanner: A Tool to Estimate and Justify Sample Size for Two-Group Studies</h1>
                        <h4 class="author">Marton Kovacs</h4>
            <address class="author_afil">
      1, 2<br><a class="author_email" href="mailto:#"></a><a href="mailto:marton.balazs.kovacs@gmail.com" class="email">marton.balazs.kovacs@gmail.com</a>
      </address>
                              <h4 class="author">Don van Ravenzwaaij</h4>
            <address class="author_afil">
      3<br><h4 class="author">Rink Hoekstra</h4>
            <address class="author_afil">
      3<br><h4 class="author">Balazs Aczel</h4>
            <address class="author_afil">
      1<br><div class="hidden name"><code>manuscript.Rmd</code></div>

    </address>
</address>
</address>
</div>

    
        <div class="abstract">
      <p class="abstract">Abstract</p>
      <p>Planning sample size often requires researchers to identify a statistical technique and to make several choices during their calculations. Currently, there is a lack of clear guidelines for researchers to find and use the applicable procedure. In the present tutorial, we introduce a web app and R package that offer nine different procedures to determine and justify the sample size for independent two-group study designs. The application highlights the most important decision points for each procedure and suggests example justifications for them. The resulting sample size report can serve as a template for preregistrations and manuscripts.</p>
      <!-- https://tinyurl.com/ybremelq -->
    </div>
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Seed for random number generation</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span>
<span class="fu">knitr</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/knitr/man/opts_chunk.html">opts_chunk</a></span><span class="op">$</span><span class="fu">set</span><span class="op">(</span>cache.extra <span class="op">=</span> <span class="fu">knitr</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/knitr/man/rand_seed.html">rand_seed</a></span>, tidy.opts <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>width.cutoff <span class="op">=</span> <span class="fl">60</span><span class="op">)</span>, tidy <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>Social and behavioral sciences are known to be plagued by undersampling <span class="citation">(Ioannidis 2005)</span>. In the traditional statistical framework, even when the effect exists, undersampled studies yield either nonsignificant results or significant results due to overestimating the size of the effect. Because nonsignificant results are less likely to reach publications than significant ones, results of undersampled studies either remain unpublished or impose a substantial bias on our body of published empirical findings. In addition, the low informational value of undersampled studies may not justify the cost or potential risk they induce <span class="citation">(Halpern, Karlawish, and Berlin 2002)</span>. To mitigate these issues, authors are increasingly expected to plan and justify the sample size of their study <span class="citation">(Maxwell 2004)</span>. However, such sample size justifications are only meaningful if they provide sufficient information to the readers to judge the adequacy of the author’s decisions.</p>
<p>In the statistical literature, a few methods have been proposed to determine and justify sample size. In practice, however, authors are short of practical guides on how to navigate among the different sample size methods. The aim of our tutorial is to point out for each method the essential decision points that a researcher has to face during this process. We also provide a collection of ready-to-use analysis code and a ShinyApp that helps researchers use and report the main sample size estimation techniques for different scenarios. The tutorial is focused exclusively on the scenario of the comparison of two independent groups (i.e., the independent <em>t</em>-test design) with a one-sided test.</p>
</div>
<div id="sample-size-determination-and-justification" class="section level1">
<h1 class="hasAnchor">
<a href="#sample-size-determination-and-justification" class="anchor"></a>Sample Size Determination and Justification</h1>
<p>A lot of factors go into the determination of the sample size for an independent two group study design. In this section, we will first provide a birds-eye view of the most important decisions. Next, we will go into more detail on the specific inference tool that results from the combination of the larger choices.</p>
<p>It is crucial to not just state how we determined our planned sample size but to also give the reader insight into the reasons behind our choices. In a recent overview, <span class="citation">Lakens (2021)</span> lists six types of general approaches to justify sample size in quantitative empirical studies: (1) Measure entire population; (2) Resource constraints; (3) A-priori power analysis; (4) Accuracy; (5) Heuristics; and (6) No justification. For the first approach, no quantitative justification is necessary; and for the second approach, the researcher has no freedom to increase the sample size. Power analysis, or more generally the estimation of true positive rate, is used when one plans to conduct hypothesis testing; accuracy justifications are used when one plans to conduct parameter estimation. Our tutorial mainly focuses on approaches two, three, and four, and is aimed at providing a hands-on approach for the mechanical part of the sample size determination (i.e., the calculation). For a deeper discussion of justification of these approaches, or for other approaches (i.e., using heuristics or not providing justification), we refer the reader to <span class="citation">Lakens (2021)</span>.</p>
<div id="choosing-a-method-in-case-of-sample-size-justification" class="section level2">
<h2 class="hasAnchor">
<a href="#choosing-a-method-in-case-of-sample-size-justification" class="anchor"></a>Choosing a method in case of sample size justification</h2>
<p>In an ideal world, the choice for the number of participants would be solely determined by scientific considerations, and depending on the chosen technique the collection of data would continue until either the desired sample size or a desired outcome has been reached. In practice, researchers are limited by time (collecting data is quite demanding), money (participants or people collecting the data may be paid, and the same may hold for renting space or equipment), or availability of participants (the population may be relatively small, and/or the participation rate quite low).</p>
<p>When constrained by limited resources, it is important to be transparent about those limitations. It is also important to be open about scientific considerations. Depending on the nature of the study (perhaps it is an initial exploration?), small sample sizes need not be a dealbreaker. So although more data are always preferred from an informational point of view, by owning the limitations of our study, we improve future readers’ understanding of the process leading up to the eventual paper, and we also answer in advance to those who think the chosen sample size was insufficient.</p>
<p>Whether or not authors have limited resources, two important choices need to be made: (1) whether they are interested in <em>statistical testing</em> or in <em>parameter estimation</em>; and (2) whether they want to conduct their statistical inference within the <em>frequentist</em> framework or within the <em>Bayesian</em> framework. Starting with the first decision, statistical testing is the primary framework when one is interested in establishing whether an underlying population effect is equal to, different from, larger than, or smaller than a certain value. In essence, statistical testing lends itself to binary decision making. Typically, testing is concerned with a fixed point null hypothesis (e.g., there is no difference between two groups), although using intervals for testing is also possible. Alternatively, one might be interested in parameter estimation that is less interested in establishing the existence of a difference and instead is concerned with establishing the magnitude of the difference.</p>
<p>The second important decision concerns the statistical framework. Choosing to conduct statistical tests within a frequentist framework, one is usually interested in balancing the type I (false positive) and type II (false negative) error rates. Practitioners choosing to conduct statistical tests within a Bayesian framework are typically interested in being able to quantify the relative probability of hypotheses or models being true given the data and in including prior information.</p>
<p>Within the realm of statistical testing, there are some other factors that affect the preferred inference tool: Do you prefer to test for equivalence (no difference in mean) or for superiority (mean of one group larger than mean of other group), are you interested in calculating a required sample size for a specific hypothetical effect size or for a range of possible values, and do you wish to employ sequential testing (applicable to Bayesian testing)? For frequentist estimation, the preferred inference tool might differ depending on whether we evaluate uncertainty for each group separately or jointly. We will describe these specific factors when we go into detail about each of the preferred methods. A flow-chart representing all of these choices is given in Figure @ref(fig:flowchart).</p>
<div class="figure" style="text-align: center">
<img src="flowchart.jpg" alt="(ref:flowchart-caption)" width="100%"><p class="caption">
(ref:flowchart-caption)
</p>
</div>
<p>(ref:flowchart-caption) The figure depicts the decisions that one faces when choosing among sample size estimation methods. The nine sample size estimation methods discussed in this paper are listed in the bottom row. Some decisions are determined by the investigated question and the design of the study while others are based on the preferred statistical framework.</p>
</div>
<div id="how-to-use-this-guide" class="section level2">
<h2 class="hasAnchor">
<a href="#how-to-use-this-guide" class="anchor"></a>How to use this guide</h2>
<p>In the next section, we will illustrate the specific inference tools and resulting sample size calculations in more detail using a ShinyApp and an R package we have developed. Throughout this section, we recurrently use two terms that have different meanings for different techniques. These are the <em>true positive rate</em> (TPR), and the <em>equivalence band</em> (EqBand). The TPR reflects the long-run probability of concluding there is an effect, given that it does exist. For traditional null hypothesis testing, this is typically referred to as power, but related concepts exist for different inference tools. The EqBand refers to an effect size region, typically around zero, that is deemed clinically insignificant or irrelevant. Different names are given to this region depending on the technique that employs them, such as statistical effect size of interest (SESOI) or region of practical equivalence (ROPE). For both TPR and EqBand, we explain the specific meaning in context of the relevant inference tool below.</p>
<p>For each method, only the main parameters can be adjusted with a certain range of values in the ShinyApp by using a slider. These parameters are presented in the text in bold. Other parameters are set to preset values in the application but can be adjusted in the accompanying R package to any sensible value. These parameters are highlighted in italics in the tutorial. Both the app and the package allow the users to save or copy a text template with the results of the sample size determination. We offer a list of possible justifications at the decision points for each method (indicated between square brackets), but users are able to provide their own justification as free-text. The provided justification text could serve as a stub for the description of the chosen sample size in a paper, a preregistration or registered report, or a grant proposal.</p>
<p>Throughout, we will use the example story of Mary the educational psychologist. Mary has come up with a new set of games that challenge spatial insight. She believes that distributed and targeted engagement with these games for a period of six months for children in the age range of 8 to 12 will lead to lasting improvements on their IQ score as measured through Raven’s progressive matrices test (population mean 100, population SD 15). Mary collects data for a control sample that gets regular education and for an experimental sample and plans to compare those samples. For illustrative purposes, we will have Mary’s hypothetical study goal depend on the earlier presented decisions to illustrate different scenarios for each sample size planning method.</p>
<p>The ShinyApp is available on <a href="https://martonbalazskovacs.shinyapps.io/SampleSizePlanner">https://martonbalazskovacs.shinyapps.io/SampleSizePlanner</a> and the R package can be installed by running the following command in R <code>devtools::install_github("marton-balazs-kovacs/SampleSizePlanner")</code>. There is more information about the R package and the ShinyApp on the projects’ Github page <a href="https://github.com/marton-balazs-kovacs/SampleSizePlanner">https://github.com/marton-balazs-kovacs/SampleSizePlanner</a>, or on the website <a href="https://marton-balazs-kovacs.github.io/SampleSizePlanner/">https://marton-balazs-kovacs.github.io/SampleSizePlanner/</a>.</p>
</div>
<div id="testing" class="section level2">
<h2 class="hasAnchor">
<a href="#testing" class="anchor"></a>1. Testing</h2>
<div id="effect-size-0" class="section level3">
<h3 class="hasAnchor">
<a href="#effect-size-0" class="anchor"></a>1.1. Effect size = 0</h3>
<div id="two-onesided-tests-tost" class="section level4">
<h4 class="hasAnchor">
<a href="#two-onesided-tests-tost" class="anchor"></a>1.1.1. Two One‐Sided Tests (TOST)</h4>
<div id="study-context" class="section level5">
<h5 class="hasAnchor">
<a href="#study-context" class="anchor"></a>Study context</h5>
<p>Mary would like to know what sample size she needs for a power of .80 to study whether the mean IQ score of the experimental group’s population is practically equivalent to the mean IQ score of the control group. She tests this assumption in a frequentist framework, and considers a population effect size between -0.2 and 0.2 to be ‘practically equivalent’ to no difference. This would correspond to IQ scores between 97 (100+15*-.2) and 103 (100+15*.2).</p>
</div>
<div id="description" class="section level5">
<h5 class="hasAnchor">
<a href="#description" class="anchor"></a>Description</h5>
<p>TOST is a frequentist equivalence testing approach that adopts two one-sided hypotheses to designate an interval hypothesis <span class="citation">(Schuirmann 1987)</span>. The lower and upper boundaries of the interval are determined by the equivalence band (i.e. SESOI) around the expected population effect size (e.g., 0). <span class="citation">Lakens, Scheel, and Isager (2018)</span> lists several methods that can be used to determine the SESOI . In case of TOST, the two null hypotheses state that the effect size is equal to the lower and upper equivalence band values, whereas the alternative hypotheses state that the effect size is significantly smaller than the upper equivalence band value and significantly larger than the lower equivalence band value. In case both one-sided tests reject the null-hypotheses at a given significance level, the group means are considered to be practically equivalent. See <span class="citation">Lakens, Scheel, and Isager (2018)</span>, for further reading.</p>
</div>
<div id="parameters" class="section level5">
<h5 class="hasAnchor">
<a href="#parameters" class="anchor"></a>Parameters</h5>
<p><strong>Delta:</strong> The expected population effect size. In most cases, this value will be zero.<br><strong>TPR:</strong> The desired long run probability of obtaining a significant result with TOST, given Delta.<br><strong>EqBand:</strong> The chosen width of the region for practical equivalence, i.e. the SESOI.<br><em>Alpha:</em> The level of significance. The alpha level in the application is preset to 0.05.<br></p>
</div>
<div id="how-to-use-the-package" class="section level5">
<h5 class="hasAnchor">
<a href="#how-to-use-the-package" class="anchor"></a>How to use the package</h5>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">SampleSizePlanner</span><span class="fu">::</span><span class="fu"><a href="../reference/ssp_tost.html">ssp_tost</a></span><span class="op">(</span>tpr <span class="op">=</span> <span class="fl">0.8</span>, eq_band <span class="op">=</span> <span class="fl">0.2</span>, delta <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></code></pre></div>
</div>
<div id="how-to-report-your-sample-size-estimation" class="section level5">
<h5 class="hasAnchor">
<a href="#how-to-report-your-sample-size-estimation" class="anchor"></a>How to report your sample size estimation</h5>
<p>In order to calculate an appropriate sample size for testing whether the two groups are practically equivalent, we used the Two One-Sided Tests of Equivalence <span class="citation">(TOST; Schuirmann 1987)</span> method. We set the aimed TPR to be 0.8, because [1) it is the common standard in the field; 2) it is the journal publishing requirement]. We consider all effect sizes below 0.2 equivalent to zero, because [1) previous studies reported a similar equivalence band; 2) of the following substantive reasons: …]. The expected delta was 0 because [1) we expected no difference between the groups]. Based on these parameters, a sample size of 429 per group was estimated in order to reach a TPR of 0.8 with our design.</p>
</div>
</div>
<div id="equivalence-interval-bayes-factor" class="section level4">
<h4 class="hasAnchor">
<a href="#equivalence-interval-bayes-factor" class="anchor"></a>1.1.2. Equivalence interval Bayes factor</h4>
<div id="study-context-1" class="section level5">
<h5 class="hasAnchor">
<a href="#study-context-1" class="anchor"></a>Study context</h5>
<p>Mary would like to know what sample size she needs to have a long-run probability of .80 of obtaining a Bayes factor larger than 10. Mary would like to test whether the mean IQ score of the experimental group’s population is practically equivalent to the mean IQ score of the control group. Mary hypothesizes that there is no difference (i.e., H<sub>0</sub> is true). Mary tests this assumption in a Bayesian framework. Mary considers a population effect size between -0.2 and under 0.2 to be ‘practically equivalent’. This would correspond to IQ scores between 97 (100+15*-.2) and 103 (100+15*.2).</p>
</div>
<div id="description-1" class="section level5">
<h5 class="hasAnchor">
<a href="#description-1" class="anchor"></a>Description</h5>
<p>Equivalence interval Bayes factors contrast an equivalence hypothesis to a non-equivalence hypothesis and quantify the evidence with Bayes factors. Typically, H<sub>0</sub> constitutes the equivalence interval (comparable to SESOI in the TOST framework), and H<sub>a</sub> constitutes the complementary non-equivalence regions. Formally, the Bayes factor is calculated by dividing the fraction <em>posterior area inside the interval/posterior area outside the interval</em> (i.e., the posterior odds) by the fraction <em>prior area inside the interval/prior area outside the interval</em> (i.e., the prior odds). The resulting value quantifies how much more likely it is that the data occurred under a population effect size deemed ‘equivalent’ relative to the data having occurred under a population effect size deemed non-equivalent. The current implementation uses a default Cauchy prior on effect size with scale parameter 1/<span class="math inline">\(\sqrt{2}\)</span>. For further reading, see <span class="citation">Morey and Rouder (2011)</span> and <span class="citation">Ravenzwaaij et al. (2019)</span>.</p>
</div>
<div id="parameters-1" class="section level5">
<h5 class="hasAnchor">
<a href="#parameters-1" class="anchor"></a>Parameters</h5>
<p><strong>Delta:</strong> The expected population effect size.<br><strong>TPR:</strong> The desired long-run probability of obtaining a Bayes factor at least as high as the Threshold, given Delta.<br><strong>EqBand:</strong> The chosen width of the equivalence region.<br><em>Threshold:</em> Critical threshold for the Bayes factor. The threshold level in the application can be set to 10, 6, or 3.<br></p>
</div>
<div id="how-to-use-the-package-1" class="section level5">
<h5 class="hasAnchor">
<a href="#how-to-use-the-package-1" class="anchor"></a>How to use the package</h5>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">SampleSizePlanner</span><span class="fu">::</span><span class="fu"><a href="../reference/ssp_eq_bf.html">ssp_eq_bf</a></span><span class="op">(</span>tpr <span class="op">=</span> <span class="fl">0.8</span>, delta <span class="op">=</span> <span class="fl">0</span>, eq_band <span class="op">=</span> <span class="fl">0.2</span>,
    thresh <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></code></pre></div>
</div>
<div id="how-to-report-your-sample-size-estimation-1" class="section level5">
<h5 class="hasAnchor">
<a href="#how-to-report-your-sample-size-estimation-1" class="anchor"></a>How to report your sample size estimation</h5>
<p>In order to estimate the sample size, we used the interval equivalent Bayes factor <span class="citation">(Morey and Rouder 2011; Ravenzwaaij et al. 2019)</span> method. We set the aimed TPR at 0.8, because [1) it is the common standard in the field; 2) it is the journal publishing requirement]. We consider all effect sizes below 0.2 equivalent to zero, because [1) previous studies reported a similar equivalence region; 2) of the following substantive reasons: …]. The expected delta was 0 because [1) we expected no difference between the groups]. Our Bayes factor threshold for concluding equivalence was 10. Based on these parameters, a minimal sample size of 144 per group was estimated in order to reach 0.8 TPR for our design.</p>
</div>
</div>
</div>
<div id="effect-size-0-1" class="section level3">
<h3 class="hasAnchor">
<a href="#effect-size-0-1" class="anchor"></a>1.2. Effect size &gt;0</h3>
<div id="frequentist" class="section level4">
<h4 class="hasAnchor">
<a href="#frequentist" class="anchor"></a>1.2.1. Frequentist</h4>
<div id="classical-power-analysis" class="section level5">
<h5 class="hasAnchor">
<a href="#classical-power-analysis" class="anchor"></a>1.2.1.1. Classical power analysis</h5>
<div id="study-context-2" class="section level6">
<h6>Study context</h6>
<p>Mary would like to know what sample size she needs for a power of .80 to study whether the mean IQ score of the experimental group’s population is significantly higher than the mean IQ score of the control group. She tests this assumption in a frequentist framework for a hypothetical population effect size of 0.5. This corresponds to a mean IQ score of 107.5 in the experimental group (100+15*.5), assuming a mean IQ score of 100 in the control group.</p>
</div>
<div id="description-2" class="section level6">
<h6>Description</h6>
<p>The classical power analysis approach allows to calculate the required sample size in order to obtain a significant result for the null hypothesis test a certain proportion of times in the long run given an assumed population effect size.</p>
</div>
<div id="parameters-2" class="section level6">
<h6>Parameters</h6>
<p><strong>Delta:</strong> The expected population effect size.<br><strong>TPR:</strong> The desired long-run probability of obtaining a significant result with a one-sided <em>t</em>-test, given Delta.<br><strong>Maximum N:</strong> The maximum number of participants per group (both groups are assumed to have equal sample size).<br><em>Alpha:</em> The level of significance. Alpha is preset to 0.05 in the application.<br></p>
</div>
<div id="how-to-use-the-package-2" class="section level6">
<h6>How to use the package</h6>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">SampleSizePlanner</span><span class="fu">::</span><span class="fu"><a href="../reference/ssp_power_traditional.html">ssp_power_traditional</a></span><span class="op">(</span>tpr <span class="op">=</span> <span class="fl">0.8</span>, delta <span class="op">=</span> <span class="fl">0.5</span>,
    max_n <span class="op">=</span> <span class="fl">5000</span>, alpha <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span></code></pre></div>
</div>
<div id="how-to-report-your-sample-size-estimation-2" class="section level6">
<h6>How to report your sample size estimation</h6>
<p>We used a power analysis to estimate the sample size. We set the aimed TPR at 0.8, because [1) it is the common standard in the field; 2) it is the journal publishing requirement]. The expected delta was 0.5 because [1) previous results published in …; 2) of the following substantive reasons: …]. Based on these parameters, a minimal sample size of 51 per group was estimated in order to reach 0.8 TPR for our design.</p>
</div>
</div>
<div id="power-curve" class="section level5">
<h5 class="hasAnchor">
<a href="#power-curve" class="anchor"></a>1.2.1.2. Power curve</h5>
<div id="study-context-3" class="section level6">
<h6>Study context</h6>
<p>Mary would like to know what sample size she needs for a power of .80 to study whether the mean IQ score of the experimental group’s population is significantly higher than the mean IQ score of the control group. She tests this assumption in a frequentist framework. However, she is reluctant to commit to a single hypothetical population effect size a-priori, preferring to calculate required sample size for a range of hypothetical deltas between 0.1 and 0.9.</p>
</div>
<div id="description-3" class="section level6">
<h6>Description</h6>
<p>The power curve method is similar to a classical power analysis but instead of calculating the appropriate sample size for one hypothesized population effect size, the method calculates the required sample size for a range of plausible population effect sizes.</p>
</div>
<div id="parameters-3" class="section level6">
<h6>Parameters</h6>
<p><strong>Delta:</strong> A range of hypothetical population effect sizes.<br><strong>TPR:</strong> The desired long-run probabilities of obtaining a significant result with a one-sided <em>t</em>-test, given each value of Delta.<br><strong>Maximum N:</strong> The maximum number of participants per group (both groups are assumed to have equal sample size).<br><em>Alpha:</em> The level of significance. Alpha is preset to 0.05 in the application.<br></p>
</div>
<div id="how-to-use-the-package-3" class="section level6">
<h6>How to use the package</h6>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Determine the sample sizes for each delta</span>
<span class="va">curve_data</span> <span class="op">&lt;-</span> <span class="fu">SampleSizePlanner</span><span class="fu">::</span><span class="fu"><a href="../reference/ssp_power_curve.html">ssp_power_curve</a></span><span class="op">(</span>tpr <span class="op">=</span> <span class="fl">0.8</span>, delta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.1</span>,
    <span class="fl">0.9</span>, <span class="fl">0.01</span><span class="op">)</span>, max_n <span class="op">=</span> <span class="fl">5000</span><span class="op">)</span>

<span class="co"># Plot the power curve</span>
<span class="fu">SampleSizePlanner</span><span class="fu">::</span><span class="fu"><a href="../reference/plot_power_curve.html">plot_power_curve</a></span><span class="op">(</span>delta <span class="op">=</span> <span class="va">curve_data</span><span class="op">$</span><span class="va">delta</span>,
    n1 <span class="op">=</span> <span class="va">curve_data</span><span class="op">$</span><span class="va">n1</span>, animated <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></code></pre></div>
</div>
<div id="how-to-report-your-sample-size-estimation-3" class="section level6">
<h6>How to report your sample size estimation</h6>
<p>We used a power analysis to estimate the sample size. We set the aimed TPR at 0.8, because [1) it is the common standard in the field; 2) it is the journal publishing requirement]. Because [1) we have no clear expectation of the magnitude of delta 2) we expected the delta to be around…], we include power calculations for delta ranging from 0.1 to 0.9. Based on these parameters, minimal sample sizes per group for different hypothetical effect sizes to reach 0.8 TPR can be found in Figure @ref(fig:powercurve).</p>
<div class="figure" style="text-align: center">
<img src="manuscript_files/figure-html/powercurve-1.png" alt="(ref:powercurve-caption)" width="700"><p class="caption">
(ref:powercurve-caption)
</p>
</div>
<p>(ref:powercurve-caption) The figure shows the resulting power curve created by the application. The X-axis shows the range of deltas from the example, while the Y-axis shows the corresponding sample sizes determined by the power curve method.</p>
</div>
</div>
</div>
<div id="bayesian" class="section level4">
<h4 class="hasAnchor">
<a href="#bayesian" class="anchor"></a>1.2.2. Bayesian</h4>
<div id="predetermined-sample-size-with-bayes-factor" class="section level5">
<h5 class="hasAnchor">
<a href="#predetermined-sample-size-with-bayes-factor" class="anchor"></a>1.2.2.1. Predetermined sample size with Bayes factor</h5>
<div id="study-context-4" class="section level6">
<h6>Study context</h6>
<p>Mary would like to test whether the mean IQ score of the experimental group’s population is higher than the mean IQ score of the control group. She’d like to know what sample size she needs to have for a long-run probability of .80 of obtaining a Bayes factor larger than 10. Mary plans to collect all her data in one batch without testing sequentially. Mary expects the population effect size to be 0.5. This corresponds to a mean IQ score of 107.5 (100+15*.5) in the experimental group, assuming a mean IQ score of 100 in the control group.</p>
</div>
<div id="description-4" class="section level6">
<h6>Description</h6>
<p>The present method calculates the corresponding default Bayes factor for a <em>t</em>-test statistic with Cauchy prior distribution centered on zero with scale parameter 1/<span class="math inline">\(\sqrt{2}\)</span> for several sample sizes <span class="citation">(the so-called Jeffrey-Zellner-Siow Bayes factor, see e.g., Rouder et al. 2009)</span>. The function returns the optimal sample size needed to reach the TPR for a given Bayes factor threshold to detect an expected population effect size. If a range of possible population effect sizes are plausible under the given hypothesis, the function can calculate the optimal sample sizes for the given range of effect sizes and present the results in a figure (analogous to the Power Curve method).</p>
</div>
<div id="parameters-4" class="section level6">
<h6>Parameters</h6>
<p><strong>Delta:</strong> The expected population effect size or a range of expected effect sizes.<br><strong>TPR:</strong> The long-run probability of obtaining a Bayes factor at least as high as the critical threshold favoring superiority, given Delta.<br><strong>Maximum N:</strong> The maximum number of participants per group (both groups are assumed to have equal sample size).<br><strong>Threshold:</strong> Critical threshold for the Bayes factor. Three threshold levels are available in the app: 3, 6, and 10.<br></p>
</div>
<div id="how-to-use-the-package-4" class="section level6">
<h6>How to use the package</h6>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">SampleSizePlanner</span><span class="fu">::</span><span class="fu"><a href="../reference/ssp_bf_predetermined.html">ssp_bf_predetermined</a></span><span class="op">(</span>tpr <span class="op">=</span> <span class="fl">0.8</span>, delta <span class="op">=</span> <span class="fl">0.5</span>,
    thresh <span class="op">=</span> <span class="fl">10</span>, max_n <span class="op">=</span> <span class="fl">5000</span><span class="op">)</span></code></pre></div>
</div>
<div id="how-to-report-your-sample-size-estimation-4" class="section level6">
<h6>How to report your sample size estimation</h6>
<p>We used the Jeffrey-Zellner-Siow Bayes factor method to estimate the sample size. We set the aimed TPR at 0.8, because [1) it is the common standard in the field; 2) it is the journal publishing requirement]. The expected delta was 0.5 because [1) previous results published in …; of the following substantive reasons: …]. Our evidence threshold was 10. Based on these parameters, a minimal sample size of 105 per group was estimated in order to reach a 0.8 TPR for our design.</p>
</div>
</div>
<div id="bayes-factor-design-analysis-bfda" class="section level5">
<h5 class="hasAnchor">
<a href="#bayes-factor-design-analysis-bfda" class="anchor"></a>1.2.2.2. Bayes Factor Design Analysis (BFDA)</h5>
<div id="study-context-5" class="section level6">
<h6>Study context</h6>
<p>Mary would like to know what sample size she needs to have a long-run probability of .80 of obtaining a Bayes factor larger than 10. Mary would like to test whether the mean IQ score of the experimental group’s population is higher than the mean IQ score of the control group in a Bayesian framework. Mary plans to collect all her data incrementally and as such is interested in using the advantage of not testing more than strictly necessary offered by sequential testing in her Bayesian analysis. Mary expects the population effect size to be 0.5. This corresponds to a mean IQ score of 107.5 in the experimental group (100+15*.5), assuming a mean IQ score of 100 in the control group.</p>
</div>
<div id="description-5" class="section level6">
<h6>Description</h6>
<p>The description of the BFDA method is functionally identical to the one provided in section ‘Predetermined sample size with Bayes factor’, but gains in TPR due to the addition of sequential testing. In the app, H<sub>0</sub> and H<sub>a</sub> indicate the proportion of times sequential testing leads to Bayes factors providing evidence with the given threshold for the null hypothesis and for the alternative hypothesis, respectively. For further reading, see <span class="citation">Schönbrodt and Wagenmakers (2018)</span> and <span class="citation">Schönbrodt et al. (2017)</span>.</p>
</div>
<div id="parameters-5" class="section level6">
<h6>Parameters</h6>
<p><strong>Delta:</strong> The expected population effect size.<br><strong>TPR:</strong> The long run probability of obtaining a Bayes factor at least as high as the critical threshold favoring superiority, given Delta.<br><strong>Threshold:</strong> Critical threshold for the Bayes factor. Three threshold levels are available in the app: 3, 6, and 10.<br></p>
</div>
<div id="how-to-use-the-package-5" class="section level6">
<h6>How to use the package</h6>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">SampleSizePlanner</span><span class="fu">::</span><span class="fu"><a href="../reference/ssp_bfda.html">ssp_bfda</a></span><span class="op">(</span>tpr <span class="op">=</span> <span class="fl">0.8</span>, delta <span class="op">=</span> <span class="fl">0.5</span>, thresh <span class="op">=</span> <span class="fl">10</span>,
    n_rep <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span></code></pre></div>
</div>
<div id="how-to-report-your-sample-size-estimation-5" class="section level6">
<h6>How to report your sample size estimation</h6>
<p>We used the BFDA method to estimate the sample size. We set the aimed TPR at 0.8, because [1) it is the common standard in the field; 2) it is the journal publishing requirement]. The expected delta was 0.5 because [1) previous results published in …; 2) of the following substantive reasons: …]. Our evidence threshold was 10. Based on these parameters, a minimal sample size of 81 per group was estimated in order to reach a 0.8 TPR for our design.</p>
</div>
</div>
</div>
</div>
</div>
<div id="estimation" class="section level2">
<h2 class="hasAnchor">
<a href="#estimation" class="anchor"></a>2. Estimation</h2>
<div id="frequentist-1" class="section level3">
<h3 class="hasAnchor">
<a href="#frequentist-1" class="anchor"></a>2.1. Frequentist</h3>
<div id="accuracy-in-parameter-estimation-aipe" class="section level4">
<h4 class="hasAnchor">
<a href="#accuracy-in-parameter-estimation-aipe" class="anchor"></a>2.1.1. Accuracy In Parameter Estimation (AIPE)</h4>
<div id="study-context-6" class="section level5">
<h5 class="hasAnchor">
<a href="#study-context-6" class="anchor"></a>Study context</h5>
<p>Mary would like to know what sample size she needs, such that the 95% confidence interval for the population effect size has an expected width of 0.4. She estimates the population effect size to be 0.2.</p>
</div>
<div id="description-6" class="section level5">
<h5 class="hasAnchor">
<a href="#description-6" class="anchor"></a>Description</h5>
<p>Accuracy in parameter estimation aims to determine the sufficient sample size to obtain a confidence interval with a desired width (precision) around the expected effect size <span class="citation">(Kelley and Rausch 2006)</span>. Note that the width of the calculated confidence interval will depend on the sample variance. As a result, it is possible that for a given sample the variance is relatively large, leading to a resulting confidence interval that is larger than the width of the desired interval for a given sample. Thus, the AIPE method aims to establish the expected value of the calculated confidence interval, which can be thought of as the 50% long-run probability of obtaining a confidence interval no wider than the provided width.</p>
</div>
</div>
<div id="parameters-6" class="section level4">
<h4 class="hasAnchor">
<a href="#parameters-6" class="anchor"></a>Parameters</h4>
<p><strong>Delta:</strong> The expected population effect size.<br><strong>Width:</strong> The desired width of the confidence interval, given Delta.<br><strong>Confidence level:</strong> The desired level of confidence.<br></p>
<div id="how-to-use-the-package-6" class="section level5">
<h5 class="hasAnchor">
<a href="#how-to-use-the-package-6" class="anchor"></a>How to use the package</h5>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">SampleSizePlanner</span><span class="fu">::</span><span class="fu"><a href="../reference/ssp_aipe.html">ssp_aipe</a></span><span class="op">(</span>delta <span class="op">=</span> <span class="fl">0.5</span>, width <span class="op">=</span> <span class="fl">0.2</span>, confidence_level <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span></code></pre></div>
</div>
<div id="how-to-report-your-sample-size-estimation-6" class="section level5">
<h5 class="hasAnchor">
<a href="#how-to-report-your-sample-size-estimation-6" class="anchor"></a>How to report your sample size estimation</h5>
<p>In order to estimate the sample size, we used the accuracy in parameter estimation <span class="citation">(AIPE; Kelley and Rausch 2006)</span> method. We aimed for a 95% confidence level, because [1) it is the common standard in the field; 2) it is the journal publishing requirement]. The desired width was 0.4 because [1) previous studies on this topic reported a similar region of practical equivalence; 2) of the following substantive reasons: …]. We expected an underlying population effect size of 0.3, because [1) previous results published in …; 2) of the following substantive reasons: …]. Based on these parameters, a minimal sample size of 195 per group was estimated for our design.</p>
</div>
</div>
<div id="a-priori-precision-app" class="section level4">
<h4 class="hasAnchor">
<a href="#a-priori-precision-app" class="anchor"></a>2.1.2. A Priori Precision (APP)</h4>
<div id="study-context-7" class="section level6">
<h6>Study context</h6>
<p>Mary would like to know the sample size for which she will have a 95% long-run probability that the sample means in both the experimental and the control group lie within 0.2 standard deviations (3 IQ points) of the true population mean.</p>
</div>
<div id="description-7" class="section level6">
<h6>Description</h6>
<p>APP aims to determine the sample size needed to have a certain long-run probability of both sample means being within a certain range of their respective population means, expressed in terms of standard deviations <span class="citation">(Trafimow and MacDonald 2017)</span>. As a result, APP is not reliant on the expected effect size.</p>
</div>
<div id="parameters-7" class="section level5">
<h5 class="hasAnchor">
<a href="#parameters-7" class="anchor"></a>Parameters</h5>
<p><strong>Closeness:</strong> The desired closeness of the sample mean to the population mean defined in standard deviation.<br><strong>Confidence:</strong> The desired probability of obtaining the sample mean with the desired closeness to the population mean.<br></p>
<div id="how-to-use-the-package-7" class="section level6">
<h6>How to use the package</h6>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">SampleSizePlanner</span><span class="fu">::</span><span class="fu"><a href="../reference/ssp_app.html">ssp_app</a></span><span class="op">(</span>closeness <span class="op">=</span> <span class="fl">0.2</span>, confidence <span class="op">=</span> <span class="fl">0.95</span><span class="op">)</span></code></pre></div>
</div>
<div id="how-to-report-your-sample-size-estimation-7" class="section level6">
<h6>How to report your sample size estimation</h6>
<p>In order to estimate the sample size, we used the a-priori precision <span class="citation">(APP; Trafimow and MacDonald 2017)</span> method. Before data collection, we wanted to be 95% confident that both sample means lie within 0.2 SD of the true population means. Based on these parameters, the resulting minimum sample size was 126 per group for our design.</p>
</div>
</div>
</div>
</div>
<div id="bayesian-testimation" class="section level3">
<h3 class="hasAnchor">
<a href="#bayesian-testimation" class="anchor"></a>2.2. Bayesian testimation</h3>
<div id="region-of-practical-equivalence-rope" class="section level4">
<h4 class="hasAnchor">
<a href="#region-of-practical-equivalence-rope" class="anchor"></a>2.2.1. Region of Practical Equivalence (ROPE)</h4>
<div id="study-context-8" class="section level6">
<h6>Study context</h6>
<p>Mary would like to conduct parameter estimation to see whether the mean IQ score of her experimental group’s population is practically equivalent to 100. She would like to know what sample size she needs to have a long-run probability of .80 of obtaining a 95% highest density interval that is contained within her predefined region of practical equivalence (ROPE). Mary hypothesizes that there is no difference (i.e., H<sub>0</sub> is true). She considers a population effect size between -0.2 and under 0.2 to be ‘practically equivalent’. This would correspond to IQ scores between 97 (100+15*-.2) and 103 (100+15*.2).</p>
</div>
<div id="description-8" class="section level6">
<h6>Description</h6>
<p>The highest density interval region of practical equivalence technique (HDI-ROPE, often just referred to as ROPE) shares some features with the equivalence interval Bayes factor procedure. Both define an equivalence interval, construct a prior for the population effect size, and update to a posterior after the data comes in. The equivalence interval Bayes factor procedure then focuses on the posterior and prior odds under complementary hypotheses. The ROPE procedure, on the other hand, identifies the 95% highest density interval (HDI; other percentages are permissible as well) and determines whether or not the HDI is fully contained within the equivalence interval. For further reading, see <span class="citation">Kruschke (2018)</span> and <span class="citation">Kruschke (2011)</span>.</p>
</div>
<div id="parameters-8" class="section level6">
<h6>Parameters</h6>
<p><strong>Delta:</strong> The expected population effect size.<br><strong>TPR:</strong> The desired long run probability of having the HDI fully contained within the ROPE interval, given Delta.<br><strong>EqBand:</strong> The chosen ROPE interval.<br></p>
</div>
<div id="how-to-use-the-package-8" class="section level6">
<h6>How to use the package</h6>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">SampleSizePlanner</span><span class="fu">::</span><span class="fu"><a href="../reference/ssp_bf_predetermined.html">ssp_bf_predetermined</a></span><span class="op">(</span>tpr <span class="op">=</span> <span class="fl">0.8</span>, delta <span class="op">=</span> <span class="fl">0.5</span>,
    thresh <span class="op">=</span> <span class="fl">10</span>, max_n <span class="op">=</span> <span class="fl">5000</span><span class="op">)</span></code></pre></div>
</div>
<div id="how-to-report-your-sample-size-estimation-8" class="section level6">
<h6>How to report your sample size estimation</h6>
<p>In order to estimate the sample size, we used the Region of Practical Equivalence <span class="citation">(Kruschke 2018)</span> method. We set the aimed TPR at 0.8, because [1) it is the common standard in the field; 2) it is the journal publishing requirement]. We consider all effect sizes below 0.2 equivalent to zero, because [1) previous studies reported a similar region of practical equivalence; 2) of the following substantive reasons: …]. The expected delta was 0 because [1) we expected no difference between the groups]. Based on these parameters, a minimal sample size of 517 per group was estimated in order to reach a 0.8 TPR for our design.</p>
</div>
</div>
</div>
</div>
</div>
<div id="summary" class="section level1">
<h1 class="hasAnchor">
<a href="#summary" class="anchor"></a>Summary</h1>
<p>Justifying the decisions made during the sample size planning process presents valuable information when one evaluates the inferences drawn from a study. The Shiny app and R package presented in this paper aim to help researchers to choose and employ their sample size estimation method. In addition, the tool provides assistance in reporting the process and justification behind sample size choices. We encourage users and experts of the field to provide feedback and recommendations towards further developments.</p>
</div>
<div id="authors-contribution" class="section level1">
<h1 class="hasAnchor">
<a href="#authors-contribution" class="anchor"></a>Authors contribution</h1>
<p><strong>Conceptualization:</strong> Marton Kovacs, Don van Ravenzwaaij, Rink Hoekstra, and Balazs Aczel.<br><strong>Methodology:</strong> Don van Ravenzwaaij.<br><strong>Project Administration:</strong> Marton Kovacs.<br><strong>Software:</strong> Marton Kovacs and Don van Ravenzwaaij.<br><strong>Supervision:</strong> Balazs Aczel.<br><strong>Writing - Original Draft Preparation:</strong> Marton Kovacs, Don van Ravenzwaaij, Rink Hoekstra, and Balazs Aczel.<br><strong>Writing - Review &amp; Editing:</strong> Marton Kovacs, Don van Ravenzwaaij, Rink Hoekstra, and Balazs Aczel.</p>
</div>
<div id="notes" class="section level1">
<h1 class="hasAnchor">
<a href="#notes" class="anchor"></a>Notes</h1>
<div id="glossary" class="section level2">
<h2 class="hasAnchor">
<a href="#glossary" class="anchor"></a>Glossary</h2>
<p><em>Accuracy in Parameter Estimation (AIPE):</em> A sample size estimation method used for parameter estimation. The approach aims to find the required sample size, such that the confidence interval has a certain expected width.<br><em>Priori Procedure (APP):</em> The approach aims to plan a sample size based on how close the researcher wishes both sample means to be to their respective population parameter, and how confident the researcher wants to be in this.<br><em>Bayesian inference:</em> A general framework for updating one’s prior beliefs in light of new data.<br><em>Bayes Factor Design Analysis (BFDA):</em> This technique provides an expected sample size such that compelling evidence in the form of a Bayes factor can be collected for a given effect size with a certain long-run probability when allowing for sequential testing.<br><em>Testing vs. Estimation:</em> Two schools of inference, focusing on establishing whether or not an effect exists versus establishing the magnitude of an effect, respectively.<br><em>Equivalence band (EqBand):</em> The region of effect sizes considered practically equivalent to zero. In our paper, SESOI and ROPE are subsumed under EqBand.<br><em>Frequentist inference:</em> A general framework in which probabilities are defined as frequencies in hypothetical repeated events. In context of statistical testing, frequentist inference is concerned with long-run error rates of rejecting the null hypothesis for the observed or more extreme parameters in a given design when the model assumptions (e.g., independence of observations) are true.<br><em>Statistical power:</em> The long-run probability of finding a significant effect given a certain population effect size.<br><em>True positive rate (TPR):</em> The long-run probability of finding evidence for an effect, given that it exists. In our paper, statistical power is subsumed under TPR.<br><em>Classical power analysis:</em> This method is used to estimate the minimum sample size that a design needs to reach a certain level of statistical power, given a desired significance level and expected effect size.<br><em>Power-curve:</em> This curve shows how changes in effect size modify the statistical power of a test.<br><em>Region Of Practical Equivalence (ROPE):</em> The region of effect sizes considered practically equivalent to zero under the HDI-ROPE method.<br><em>Smallest Effect Size Of Interest (SESOI):</em> The region of effect sizes considered practically equivalent to zero under the TOST method.<br><em>Sequential testing:</em> The practice of incrementally testing as data comes in, typically until some pre-determined level of evidence is obtained.<br><em>Two One‐Sided Tests (TOST):</em> A frequentist statistical testing approach aimed at establishing equivalence between two groups.<br><em>Equivalence interval BF:</em> A Bayesian statistical testing approach aimed at establishing equivalence between two groups.<br></p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="references" class="section level1">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">r_refs</span><span class="op">(</span>file <span class="op">=</span> <span class="st">"ssp.bib"</span><span class="op">)</span></code></pre></div>

<div id="refs" custom-style="Bibliography">
<div id="ref-halpern_continuing_2002">
<p>Halpern, Scott D., Jason HT Karlawish, and Jesse A. Berlin. 2002. “The Continuing Unethical Conduct of Underpowered Clinical Trials.” <em>Jama</em> 288 (3): 358–62.</p>
</div>
<div id="ref-ioannidis_why_2005">
<p>Ioannidis, John PA. 2005. “Why Most Published Research Findings Are False.” <em>PLoS Medicine</em> 2 (8): e124.</p>
</div>
<div id="ref-kelley_sample_2006">
<p>Kelley, Ken, and Joseph R. Rausch. 2006. “Sample Size Planning for the Standardized Mean Difference: Accuracy in Parameter Estimation via Narrow Confidence Intervals.” <em>Psychological Methods</em> 11 (4): 363–85.</p>
</div>
<div id="ref-kruschke_bayesian_2011">
<p>Kruschke, John K. 2011. “Bayesian Assessment of Null Values via Parameter Estimation and Model Comparison.” <em>Perspectives on Psychological Science</em> 6 (3): 299–312.</p>
</div>
<div id="ref-kruschke_rejecting_2018">
<p>———. 2018. “Rejecting or Accepting Parameter Values in Bayesian Estimation.” <em>Advances in Methods and Practices in Psychological Science</em> 1 (2): 270–80.</p>
</div>
<div id="ref-lakens_sample_2021">
<p>Lakens, Daniel. 2021. “Sample Size Justification,” January. <a href="https://doi.org/10.31234/osf.io/9d3yf">https://doi.org/10.31234/osf.io/9d3yf</a>.</p>
</div>
<div id="ref-lakens_equivalence_2018">
<p>Lakens, Daniel, Anne M. Scheel, and Peder M. Isager. 2018. “Equivalence Testing for Psychological Research: A Tutorial.” <em>Advances in Methods and Practices in Psychological Science</em> 1 (2): 259–69.</p>
</div>
<div id="ref-maxwell_persistence_2004">
<p>Maxwell, Scott E. 2004. “The Persistence of Underpowered Studies in Psychological Research: Causes, Consequences, and Remedies.” <em>Psychological Methods</em> 9 (2): 147–63.</p>
</div>
<div id="ref-morey_bayes_2011">
<p>Morey, Richard D., and Jeffrey N. Rouder. 2011. “Bayes Factor Approaches for Testing Interval Null Hypotheses.” <em>Psychological Methods</em> 16 (4): 406–19.</p>
</div>
<div id="ref-van_ravenzwaaij_bayes_2019">
<p>Ravenzwaaij, Don van, Rei Monden, Jorge N. Tendeiro, and John PA Ioannidis. 2019. “Bayes Factors for Superiority, Non-Inferiority, and Equivalence Designs.” <em>BMC Medical Research Methodology</em> 19 (1): 1–12.</p>
</div>
<div id="ref-rouder_bayesian_2009">
<p>Rouder, Jeffrey N., Paul L. Speckman, Dongchu Sun, Richard D. Morey, and Geoffrey Iverson. 2009. “Bayesian T Tests for Accepting and Rejecting the Null Hypothesis.” <em>Psychonomic Bulletin &amp; Review</em> 16 (2): 225–37.</p>
</div>
<div id="ref-schonbrodt_bayes_2018">
<p>Schönbrodt, Felix D., and Eric-Jan Wagenmakers. 2018. “Bayes Factor Design Analysis: Planning for Compelling Evidence.” <em>Psychonomic Bulletin &amp; Review</em> 25 (1): 128–42.</p>
</div>
<div id="ref-schonbrodt_sequential_2017">
<p>Schönbrodt, Felix D., Eric-Jan Wagenmakers, Michael Zehetleitner, and Marco Perugini. 2017. “Sequential Hypothesis Testing with Bayes Factors: Efficiently Testing Mean Differences.” <em>Psychological Methods</em> 22 (2): 322–39.</p>
</div>
<div id="ref-schuirmann_comparison_1987">
<p>Schuirmann, Donald J. 1987. “A Comparison of the Two One-Sided Tests Procedure and the Power Approach for Assessing the Equivalence of Average Bioavailability.” <em>Journal of Pharmacokinetics and Biopharmaceutics</em> 15 (6): 657–80.</p>
</div>
<div id="ref-trafimow_performing_2017">
<p>Trafimow, David, and Justin A. MacDonald. 2017. “Performing Inferential Statistics Prior to Data Collection.” <em>Educational and Psychological Measurement</em> 77 (2): 204–19.</p>
</div>
</div>

</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Marton Kovacs.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
